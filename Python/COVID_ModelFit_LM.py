# DataGrab Dependencies (not sure how to skip this)
import os
import re
import pandas as pd
import numpy as np
from datetime import date, timedelta
import datetime as dtm
import matplotlib.pyplot as plt
import seaborn as sns
import pickle

# Import DataGrab Functions
from COVID_DataGrab import *

# Import Model Fitting Functions
from collections import *
import torch


def camelCase(st):
    output = ''.join(x for x in st.title() if x.isalnum())
    return output[0].lower() + output[1:]

def negPart(x):
    return -1*torch.relu(-1*x)

class expDecayLrnRate(object):
    def __init__(self,l0,lMin,lDecay):
        self.lMin = lMin
        self.lDecay = lDecay
        self.l = max(l0,self.lMin)
    def next(self):
        self.l = max(self.l*self.lDecay, self.lMin)
        return self.l
    def __mul__(self, other):
        return(self.l*other)
    def __add__(self, other):
        return(self.l + other)
    def __float__(self):
        return self.l
    def __long__ (self):
        return self.l
    

if __name__ == '__main__':

    # Try to Download Latest Data to Directory
    covid_csv_root = os.path.expanduser(
        '~/Documents/GitHub/COVID-Experiments/CSV/')

    # Load Latest enriched CSV file generated by R
    covid = covid_enriched_load_latest(covid_csv_root)
    covid = covid.rename(dict([(x, camelCase(x))
                               for x in covid.columns]), axis='columns')

    # Subset to Training Data
    def train_subset(x): return (x.cases > 0) & (x.totCases > 1000) &\
        (x.nNonzeroRows > 20) & (x.maxGap < 4) &\
        (x.geoid != "RU")
    covid_train = covid[train_subset(covid)]

    # Generate Arrays of Training Data
    def genFeatures(x):
        y = torch.tensor(
            (x.dateInt - x.ctryMinDateInt).to_numpy(),
            dtype=torch.float64, requires_grad=False)
        return torch.stack((y**0, y**1, y**2)).T

    def genResp(x):
        return torch.tensor(np.log(x.cases.to_numpy()),
                            dtype=torch.float64, requires_grad=False)

    train_geoid_lst = covid_train.geoid.unique()
    n_geoid = len(train_geoid_lst)

    x_train = [covid_train[covid_train.geoid == id].pipe(
        genFeatures) for id in train_geoid_lst]
    y_train = [covid_train[covid_train.geoid == id].pipe(
        genResp) for id in train_geoid_lst]

    # Define Log-Density Functions

    def log_density_country(x, y, beta, rho):
        n = len(y)
        return - 0.5*torch.norm(torch.mv(x, beta.double()) - y)*(rho**2) \
            + n*torch.log(rho.abs())

    def log_density_prior(beta, mean, precChol):
        prec = torch.matmul(precChol.T, precChol)
        beta_adj = (beta - mean).unsqueeze(-1)
        return - 0.5*torch.chain_matmul(beta_adj.T, prec, beta_adj) \
            + 0.5*torch.logdet(prec)

    # Initialize Parameters
    ctryBetas = torch.randn((n_geoid, 3),
                            dtype=torch.float64, requires_grad=True)

    errorPrecRt = torch.ones((n_geoid, 1),
                             dtype=torch.float64, requires_grad=True)

    priorMean = torch.randn((3, ),
                            dtype=torch.float64, requires_grad=True)

    priorPrecChol = torch.randn((3, 3),
                                dtype=torch.float64, requires_grad=True)

    # Set learning rate params
    lrn_rate_decay = 1-1e-3
    lrn_rate_min = 1e-15
    lrn_rate = expDecayLrnRate(1e-8, lrn_rate_min, lrn_rate_decay)

    # Train the parameters
    n_iter = 50000
    print_interval = 300

    for it_num in range(n_iter):
        # Compute Likelihood
        log_lik = 0
        for ix in range(len(train_geoid_lst)):
            log_lik += \
                log_density_country(x_train[ix], y_train[ix],
                                    ctryBetas[ix], errorPrecRt[ix]) +\
                log_density_prior(ctryBetas[ix], priorMean, priorPrecChol)

        if not it_num % print_interval:
            print(it_num, log_lik.data)

        # Compute Gradient and take step
        log_lik.backward()
        with torch.no_grad():
            ctryBetas += lrn_rate * ctryBetas.grad
            ctryBetas[:, 2] = negPart(ctryBetas[:, 2])

            priorMean += lrn_rate * priorMean.grad
            priorMean[2] = negPart(priorMean[2])

            errorPrecRt += lrn_rate * errorPrecRt.grad
            errorPrecRt.data = torch.abs(errorPrecRt.data).data

            priorPrecChol += lrn_rate * priorPrecChol.grad

        lrn_rate.next()

    # Save Latest Params to Disk
    param_fname = "covid_lm_params_" \
        + dtm.datetime.now().strftime("%Y%m%d_%H%m")\
        + ".pickle"

    param_dir = os.path.expanduser(
        '~/Documents/GitHub/COVID-Experiments/ParamPickles/')

    param_write_path = os.path.join(param_dir, param_fname)

    with open(param_write_path, "wb") as handle:
        pickle.dump({'ctryBetas': ctryBetas.detach(),
                     'errorPrecRt': errorPrecRt.detach(),
                     'priorMean': priorMean.detach(),
                     'priorPrecChol': priorPrecChol.detach()},
                    handle, protocol=pickle.HIGHEST_PROTOCOL)

    # 
